# nithurshen-langsmith-MAT496
This repository documents my learning from the LangChain Academy's "Intro to LangSmith" course. Each commit corresponds to a specific video lesson and features personalized modifications to the original source code. The README file serves as a detailed log, containing a summary of key concepts from each video and notes on my code tweaks.

<b>Name:</b> K S Nithurshen  
<b>Roll No:</b> 2410110157  

## Module 0:
This was an introductory module, and it taught me how to set up my development environment for the course. I learned how to sign up for a LangSmith account and generate an API key, cloning the course's GitHub repository, creating a Python virtual environment, and installing the necessary dependencies. I learned how to configure my environment variables by creating a .env file to store my LangSmith and OpenAI API keys, preparing the environment to run the course notebooks using Jupyter. Finally, I ran a Jupyter Notebook, which showed a simple application of RAG. I modified it, and tried my own example.


## Module 1: (Visibility While Building with Tracing)
* <b>Lesson 1:</b> I've learned that the @traceable decorator from the LangSmith Python SDK is a powerful way to log function traces, as it automatically handles the RunTree lifecycle for me. The lesson also covered how to add both static metadata directly within the decorator and dynamic metadata at runtime by passing the langsmith_extra argument. I implemented these concepts in my own Retrieval-Augmented Generation (RAG) application. I applied the @traceable decorator to each function in my RAG pipeline to create a complete trace for every execution. To customize my traces, I added specific, static metadata relevant to my Formula 1 theme, such as {"context" : "It's always about F1"} and {"driver": "Sebastian Vettel"}. I also experimented with adding metadata dynamically at runtime, for instance, by passing a dictionary with information like {"races_won_in_2013": "eleven"} when calling my main RAG function.
* <b>Lesson 2:</b> Based on the lesson, I've learned how to specify different run_type values within the @traceable decorator to get specialized views in LangSmith for various operations. The course covered the correct formatting for llm, retriever, and tool runs to ensure they are rendered properly for easier debugging and analysis. I've put these concepts into practice in my own notebook by creating several F1-themed examples. For the llm run, I simulated a conversation with a race engineer and added custom metadata like {"ls_provider": "ferrari", "ls_model_name": "leclerc_memes"}. For the retriever run, I configured it to return documents with specific metadata, such as {"mercedes": "amg one"}. Finally, I successfully implemented a tool run by creating a function to check the weather at a Formula 1 circuit, like Spa, and integrated it into a larger chain that uses the tool's output to generate a final response.
* <b>Lesson 3:</b> I've learned about several alternative methods for tracing in LangSmith that offer more flexibility than just the @traceable decorator. The lesson showed that for LangChain and LangGraph applications, tracing is automatic once the environment variables are set. For more granular control, I can use the trace context manager to wrap a specific block of code, which is great when a decorator isn't practical. Finally, the wrap_openai function allows for automatically logging all OpenAI client calls without needing any decorators at all. I applied these techniques in my own F1-themed notebook. First, I built a simple RAG agent using LangGraph and saw how it was traced automatically. Then, I refactored one of my functions to use the with trace() block instead of a decorator, giving me direct control over the run's inputs and outputs. Lastly, I implemented another RAG pipeline using wrap_openai on my OpenAI client, which simplified my code by removing the need for a specific decorator on the model-calling function, and I was still able to pass custom metadata like {"team": "ferrari"}.
* <b>Lesson 4:</b> I've learned how to track multi-turn conversations in LangSmith by grouping separate traces into a single conversational thread. The lesson showed that this is done by passing a special metadata thread_id with a consistent, unique identifier for every run that's part of the same conversation. I implemented this concept in my F1-themed RAG application. First, I generated a single thread_id using the uuid library. Then, I made two distinct calls to my app, asking different F1-related questions like "How to become an F1 driver?" and "How old is Max Verstappen as of 2024?". In each call, I passed the exact same thread_id in the langsmith_extra argument, which successfully linked both traces together in the LangSmith UI, allowing me to view them as a single, continuous conversation.


## Module 2: (Testing and Evaluation)
* <b>Lesson 1:</b> I've learned that in addition to the UI, I can programmatically create and upload data to a LangSmith dataset using the Python SDK. The lesson demonstrated the process of instantiating a LangSmith Client, preparing lists of inputs and reference outputs, and then using the client.create_examples() method to bulk-upload them to a specific dataset. I applied this by creating my own custom dataset focused on Formula 1. I compiled a list of ten question-and-answer pairs about drivers like Sebastian Vettel and Max Verstappen. After formatting the questions into an inputs list and the answers into an outputs list, I successfully used the create_examples function to upload them to my designated dataset in LangSmith.
* <b>Lesson 2:</b> I've learned how to build custom evaluators in LangSmith to score the outputs of my LLM applications against reference examples. The lesson introduced the powerful LLM-as-judge technique, which uses a separate LLM to perform nuanced, semantic evaluations that go beyond simple string matching. This involves writing an evaluator function that prompts a model to score the similarity between a generated answer and a correct reference answer, often using a Pydantic model to structure the score output. I implemented this LLM-as-judge evaluator and tested it with my own custom Formula 1 examples. I created two test cases: one where the generated answer was factually incorrect but semantically similar (stating Sebastian Vettel had three championships instead of four), and another that was a direct factual contradiction (naming the wrong 2021 world champion). This allowed me to see firsthand how the evaluator could assign different scores based on the degree of semantic and factual accuracy, providing a much more sophisticated way to measure my application's performance.
* <b>Lesson 3:</b> I've learned how to run experiments in LangSmith to systematically evaluate and compare different versions of my application against a dataset. The lesson showed me how to use the langsmith.evaluate() function, which involves defining a target_function to wrap my application, specifying the data (the dataset name), and providing a list of evaluators to score the results. I put this into practice by conducting an A/B test to compare two different models. First, I ran an experiment using my langsmith_rag application with one model and evaluated its performance against my dataset using a custom is_concise_enough evaluator. Then, I modified my application to use a different model (gpt-3.5-turbo) and ran the exact same experiment again. This created two distinct experiment runs in LangSmith, allowing me to directly compare their performance metrics and decide which model version was better. I also learned about advanced features like running experiments on specific data points, setting num_repetitions for consistency, and adding metadata for better organization.
