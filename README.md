# nithurshen-langsmith-MAT496
This repository documents my learning from the LangChain Academy's "Intro to LangSmith" course. Each commit corresponds to a specific video lesson and features personalized modifications to the original source code. The README file serves as a detailed log, containing a summary of key concepts from each video and notes on my code tweaks.

<b>Name:</b> K S Nithurshen  
<b>Roll No:</b> 2410110157  

## Module 0:
This was an introductory module, and it taught me how to set up my development environment for the course. I learned how to sign up for a LangSmith account and generate an API key, cloning the course's GitHub repository, creating a Python virtual environment, and installing the necessary dependencies. I learned how to configure my environment variables by creating a .env file to store my LangSmith and OpenAI API keys, preparing the environment to run the course notebooks using Jupyter. Finally, I ran a Jupyter Notebook, which showed a simple application of RAG. I modified it, and tried my own example.


## Module 1: (Visibility While Building with Tracing)
* <b>Lesson 1:</b> I've learned that the @traceable decorator from the LangSmith Python SDK is a powerful way to log function traces, as it automatically handles the RunTree lifecycle for me. The lesson also covered how to add both static metadata directly within the decorator and dynamic metadata at runtime by passing the langsmith_extra argument. I implemented these concepts in my own Retrieval-Augmented Generation (RAG) application. I applied the @traceable decorator to each function in my RAG pipeline to create a complete trace for every execution. To customize my traces, I added specific, static metadata relevant to my Formula 1 theme, such as {"context" : "It's always about F1"} and {"driver": "Sebastian Vettel"}. I also experimented with adding metadata dynamically at runtime, for instance, by passing a dictionary with information like {"races_won_in_2013": "eleven"} when calling my main RAG function.
* <b>Lesson 2:</b> Based on the lesson, I've learned how to specify different run_type values within the @traceable decorator to get specialized views in LangSmith for various operations. The course covered the correct formatting for llm, retriever, and tool runs to ensure they are rendered properly for easier debugging and analysis. I've put these concepts into practice in my own notebook by creating several F1-themed examples. For the llm run, I simulated a conversation with a race engineer and added custom metadata like {"ls_provider": "ferrari", "ls_model_name": "leclerc_memes"}. For the retriever run, I configured it to return documents with specific metadata, such as {"mercedes": "amg one"}. Finally, I successfully implemented a tool run by creating a function to check the weather at a Formula 1 circuit, like Spa, and integrated it into a larger chain that uses the tool's output to generate a final response.
* <b>Lesson 3:</b> I've learned about several alternative methods for tracing in LangSmith that offer more flexibility than just the @traceable decorator. The lesson showed that for LangChain and LangGraph applications, tracing is automatic once the environment variables are set. For more granular control, I can use the trace context manager to wrap a specific block of code, which is great when a decorator isn't practical. Finally, the wrap_openai function allows for automatically logging all OpenAI client calls without needing any decorators at all. I applied these techniques in my own F1-themed notebook. First, I built a simple RAG agent using LangGraph and saw how it was traced automatically. Then, I refactored one of my functions to use the with trace() block instead of a decorator, giving me direct control over the run's inputs and outputs. Lastly, I implemented another RAG pipeline using wrap_openai on my OpenAI client, which simplified my code by removing the need for a specific decorator on the model-calling function, and I was still able to pass custom metadata like {"team": "ferrari"}.