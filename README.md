# nithurshen-langsmith-MAT496
This repository documents my learning from the LangChain Academy's "Intro to LangSmith" course. Each commit corresponds to a specific video lesson and features personalized modifications to the original source code. The README file serves as a detailed log, containing a summary of key concepts from each video and notes on my code tweaks.

<b>Name:</b> K S Nithurshen  
<b>Roll No:</b> 2410110157  

## Module 0:
This was an introductory module, and it taught me how to set up my development environment for the course. I learned how to sign up for a LangSmith account and generate an API key, cloning the course's GitHub repository, creating a Python virtual environment, and installing the necessary dependencies. I learned how to configure my environment variables by creating a .env file to store my LangSmith and OpenAI API keys, preparing the environment to run the course notebooks using Jupyter. Finally, I ran a Jupyter Notebook, which showed a simple application of RAG. I modified it, and tried my own example.


## Module 1: (Visibility While Building with Tracing)
* <b>Lesson 1:</b> I've learned that the @traceable decorator from the LangSmith Python SDK is a powerful way to log function traces, as it automatically handles the RunTree lifecycle for me. The lesson also covered how to add both static metadata directly within the decorator and dynamic metadata at runtime by passing the langsmith_extra argument. I implemented these concepts in my own Retrieval-Augmented Generation (RAG) application. I applied the @traceable decorator to each function in my RAG pipeline to create a complete trace for every execution. To customize my traces, I added specific, static metadata relevant to my Formula 1 theme, such as {"context" : "It's always about F1"} and {"driver": "Sebastian Vettel"}. I also experimented with adding metadata dynamically at runtime, for instance, by passing a dictionary with information like {"races_won_in_2013": "eleven"} when calling my main RAG function. (https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/main/module_1/lesson_1/my_example_of_tracing_basics.ipynb)

![l1_m1](https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/86960334aac4ac4fb0e6b1f63a814252ae8a4026/module_1/lesson_1/l1_m1.png)
* <b>Lesson 2:</b> Based on the lesson, I've learned how to specify different run_type values within the @traceable decorator to get specialized views in LangSmith for various operations. The course covered the correct formatting for llm, retriever, and tool runs to ensure they are rendered properly for easier debugging and analysis. I've put these concepts into practice in my own notebook by creating several F1-themed examples. For the llm run, I simulated a conversation with a race engineer and added custom metadata like {"ls_provider": "ferrari", "ls_model_name": "leclerc_memes"}. For the retriever run, I configured it to return documents with specific metadata, such as {"mercedes": "amg one"}. Finally, I successfully implemented a tool run by creating a function to check the weather at a Formula 1 circuit, like Spa, and integrated it into a larger chain that uses the tool's output to generate a final response. (https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/main/module_1/lesson_2/my_example_of_types_of_runs.ipynb)
* <b>Lesson 3:</b> I've learned about several alternative methods for tracing in LangSmith that offer more flexibility than just the @traceable decorator. The lesson showed that for LangChain and LangGraph applications, tracing is automatic once the environment variables are set. For more granular control, I can use the trace context manager to wrap a specific block of code, which is great when a decorator isn't practical. Finally, the wrap_openai function allows for automatically logging all OpenAI client calls without needing any decorators at all. I applied these techniques in my own F1-themed notebook. First, I built a simple RAG agent using LangGraph and saw how it was traced automatically. Then, I refactored one of my functions to use the with trace() block instead of a decorator, giving me direct control over the run's inputs and outputs. Lastly, I implemented another RAG pipeline using wrap_openai on my OpenAI client, which simplified my code by removing the need for a specific decorator on the model-calling function, and I was still able to pass custom metadata like {"team": "ferrari"}. (https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/main/module_1/lesson_3/my_example_of_alternative_tracing_methods.ipynb)
* <b>Lesson 4:</b> I've learned how to track multi-turn conversations in LangSmith by grouping separate traces into a single conversational thread. The lesson showed that this is done by passing a special metadata thread_id with a consistent, unique identifier for every run that's part of the same conversation. I implemented this concept in my F1-themed RAG application. First, I generated a single thread_id using the uuid library. Then, I made two distinct calls to my app, asking different F1-related questions like "How to become an F1 driver?" and "How old is Max Verstappen as of 2024?". In each call, I passed the exact same thread_id in the langsmith_extra argument, which successfully linked both traces together in the LangSmith UI, allowing me to view them as a single, continuous conversation. (https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/main/module_1/lesson_4/my_example_of_conversational_threads.ipynb)

![l4_m1](https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/86960334aac4ac4fb0e6b1f63a814252ae8a4026/module_1/lesson_4/l4_m1.png)


## Module 2: (Testing and Evaluation)
* <b>Lesson 1:</b> I've learned that in addition to the UI, I can programmatically create and upload data to a LangSmith dataset using the Python SDK. The lesson demonstrated the process of instantiating a LangSmith Client, preparing lists of inputs and reference outputs, and then using the client.create_examples() method to bulk-upload them to a specific dataset. I applied this by creating my own custom dataset focused on Formula 1. I compiled a list of ten question-and-answer pairs about drivers like Sebastian Vettel and Max Verstappen. After formatting the questions into an inputs list and the answers into an outputs list, I successfully used the create_examples function to upload them to my designated dataset in LangSmith. (https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/main/module_2/lesson_1/my_example_of_dataset_upload.ipynb)

![l1_m2](https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/86960334aac4ac4fb0e6b1f63a814252ae8a4026/module_2/lesson_1/l1_m2.png)
* <b>Lesson 2:</b> I've learned how to build custom evaluators in LangSmith to score the outputs of my LLM applications against reference examples. The lesson introduced the powerful LLM-as-judge technique, which uses a separate LLM to perform nuanced, semantic evaluations that go beyond simple string matching. This involves writing an evaluator function that prompts a model to score the similarity between a generated answer and a correct reference answer, often using a Pydantic model to structure the score output. I implemented this LLM-as-judge evaluator and tested it with my own custom Formula 1 examples. I created two test cases: one where the generated answer was factually incorrect but semantically similar (stating Sebastian Vettel had three championships instead of four), and another that was a direct factual contradiction (naming the wrong 2021 world champion). This allowed me to see firsthand how the evaluator could assign different scores based on the degree of semantic and factual accuracy, providing a much more sophisticated way to measure my application's performance. (https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/main/module_2/lesson_2/my_example_of_evaluators.ipynb)
* <b>Lesson 3:</b> I've learned how to run experiments in LangSmith to systematically evaluate and compare different versions of my application against a dataset. The lesson showed me how to use the langsmith.evaluate() function, which involves defining a target_function to wrap my application, specifying the data (the dataset name), and providing a list of evaluators to score the results. I put this into practice by conducting an A/B test to compare two different models. First, I ran an experiment using my langsmith_rag application with one model and evaluated its performance against my dataset using a custom is_concise_enough evaluator. Then, I modified my application to use a different model (gpt-3.5-turbo) and ran the exact same experiment again. This created two distinct experiment runs in LangSmith, allowing me to directly compare their performance metrics and decide which model version was better. I also learned about advanced features like running experiments on specific data points, setting num_repetitions for consistency, and adding metadata for better organization. (https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/main/module_2/lesson_3/experiments.ipynb)
* <b>Lesson 4:</b> I've learned how to effectively analyze and compare the A/B tests I run using the LangSmith experiments dashboard. The lesson highlighted how the platform provides a side-by-side view that makes it easy to compare aggregate statistics like latency and feedback scores across different versions of an application. The ability to filter, sort, and group results by different inputs or metadata is incredibly useful for pinpointing specific areas of success or failure. I applied these analysis techniques to the experiments I ran on my RAG application. By comparing the gpt-4o-mini and gpt-3.5-turbo experiments, I was able to see at a glance which model had better overall scores on my "is_concise" evaluator. More importantly, I learned to drill down into individual traces where the models produced different answers, allowing me to inspect the entire execution path and understand exactly why one model performed better than the other on a specific question. This granular level of analysis is crucial for debugging and making informed decisions on how to improve my application.

![l4_m2](https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/86960334aac4ac4fb0e6b1f63a814252ae8a4026/module_2/lesson_4/l4_m2.png)
* <b>Lesson 5:</b> I've learned how to conduct pairwise experiments to compare two different versions of my application, which is especially useful when I don't have a dataset with ground-truth answers. The lesson demonstrated that this involves running two separate experiments first, and then using a third, LLM-as-judge evaluator to compare their outputs side-by-side on a case-by-case basis. This "head-to-head" comparison helps determine which version performs better based on a defined preference. I applied this by setting up a summarization task. First, I created a custom summary_score_evaluator that judged the quality and verbosity of a summary and returned the harmonic mean of the scores. I then ran two separate experiments on a dataset of meeting transcripts: one with a "Good Summarizer" prompt and another with a "Bad Summarizer" prompt. Finally, I implemented a ranked_preference evaluator that acted as an impartial judge, comparing the outputs from both experiments and scoring which summary was better for each transcript, and used langsmith.evaluate to run the final pairwise comparison. (https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/main/module_2/lesson_5/my_example_of_pairwise_experiments.ipynb)

![l5_m2](https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/86960334aac4ac4fb0e6b1f63a814252ae8a4026/module_2/lesson_5/l5_m2.png)
* <b>Lesson 6:</b> I've learned about a powerful feature called summary evaluators, which are different from regular evaluators because they compute a single, aggregate score over an entire experiment rather than scoring each example individually. The lesson explained that this is perfect for metrics like accuracy or an F1 score that require analyzing the complete set of results. The key is to define a function that accepts lists of outputs and reference outputs and then pass it to the summary_evaluators parameter in the evaluate call. I applied this concept by building a classifier to label statements as 'Chaotic' or 'Not chaotic', putting my own thematic spin on the lesson's toxicity example. I then wrote an f1_score_summary_evaluator to calculate the aggregate F1 score based on the true positives, false positives, and false negatives across all the test runs. By running the experiment with this summary evaluator, I was able to generate a single, holistic F1 score that measured the overall performance of my classifier on the entire dataset. (https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/main/module_2/lesson_6/my_example_of_summary_evaluators.ipynb)

![l6_m2](https://github.com/Nithurshen/nithurshen-langsmith-MAT496/blob/86960334aac4ac4fb0e6b1f63a814252ae8a4026/module_2/lesson_6/l6_m2.png)

## Module 3: (Prompt Engineering)
* <b>Lesson 1:</b> I've learned that the LangSmith Playground is a powerful environment for iterating on and testing prompts against a variety of examples. The lesson demonstrated how to programmatically create datasets using the LangSmith Python SDK, which is a key step for structured experimentation. Specifically, I saw how to use the Client to instantiate a new dataset and then populate it with corresponding inputs and reference outputs using client.create_examples. I implemented these concepts by creating my own custom dataset for an application focused on Formula 1. I created a list of F1-themed questions and their ground-truth answers, such as ("What is Max Verstappen's car number?", "It is always 33") and ("What color is McLaren?", "McLaren is Papaya in color"), and used the SDK to upload them into a new dataset named "F1 Questions l1_m3". After creating the dataset, I loaded it into the Playground to run experiments, which provided a clear, side-by-side view to compare the model's outputs against my reference answers for each question.
* <b>Lesson 2:</b> I've learned that the LangSmith Prompt Hub is a powerful feature for managing and versioning prompts, allowing me to decouple prompt engineering from my application code. The lesson covered how to use the langchain.hub module to programmatically pull existing prompts, including specific versions, directly into a Python environment. It also demonstrated how to upload new or updated prompts using client.push_prompt. I implemented these concepts by creating my own set of Formula 1-themed prompts. I created and uploaded prompts like "f1-rag-prompt" and "f1-runnable-sequence" to my own repository. I then practiced pulling a prompt named "f1_driver_name" from the hub, which is designed to act as an FIA steward and answer questions about F1 drivers. I tested it by invoking the prompt with a driver's name, for example, {"driver": "Max Verstappen?"}, to get back the driver's car number and team, showcasing how easily I can integrate version-controlled prompts from a central repository into my application.
* <b>Lesson 3:</b> I've learned how to manage the complete prompt engineering lifecycle by combining several LangSmith features into a cohesive workflow. The lesson demonstrated how to log an initial trace from a RAG application, create a dedicated dataset for evaluation, iterate on a prompt in the Playground, and then update the application to pull the improved prompt from the Prompt Hub. I implemented this entire process for my own RAG application, which answers technical questions about GitHub. I began by creating a "Github Questions" dataset with high-quality question-and-answer pairs. Then, I used the LangSmith Playground to test and refine a new prompt, "git_terminal," against this dataset. Finally, I updated my RAG application's code to replace its original hard-coded prompt with hub.pull("git_terminal"), effectively closing the loop and deploying the improved, version-controlled prompt into my application.
* <b>Lesson 4:</b> I've learned that the LangSmith Prompt Canvas is a powerful, AI-assisted workspace for building and refining prompts. The lesson highlighted its interactive nature, allowing you to collaborate with an LLM agent by giving instructions in plain English to iteratively improve a prompt. I saw how to track these improvements with the "Diff" view and how the agent can help structure complex prompts with personas, settings, and few-shot examples. I used these features to engineer a prompt for a cyberpunk-themed chatbot. I started with a basic instruction to create a persona for a "cyberpunk human living in the year 2077, who is an outlaw trying to survive a dystopian world". The Prompt Canvas agent then helped me flesh this out into a detailed prompt, adding a gritty tone, a backstory, and example interactions. I continued to refine it by providing further instructions, like asking the agent to add more specific examples to make the character more relatable.