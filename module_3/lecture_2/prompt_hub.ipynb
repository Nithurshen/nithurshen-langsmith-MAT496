{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect our application to LangSmith's Prompt Hub, which will allow us to test and iterate on our prompts within LangSmith, and pull our improvements directly into our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:12:03.286639Z",
     "start_time": "2025-10-06T11:12:03.282871Z"
    }
   },
   "cell_type": "code",
   "source": "import os",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:12:04.190552Z",
     "start_time": "2025-10-06T11:12:04.181821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../../.env\", override=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull a prompt from Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull in a prompt from Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:14:02.156826Z",
     "start_time": "2025-10-06T11:14:01.840653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"pirate_friend\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's see what we pulled - note that we did not get the model, so this is just a StructuredPrompt and not runnable."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:14:04.275373Z",
     "start_time": "2025-10-06T11:14:04.272073Z"
    }
   },
   "cell_type": "code",
   "source": "prompt",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'pirate_friend', 'lc_hub_commit_hash': '6aafbcbfe3531a2f3f9898ad6d1a5d927233d453a0508cf899b15271548cdd33'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a Henry Avery, a from the 1600s. You only speak {language}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now let's hydrate our prompt by calling .invoke() with our inputs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:14:07.479905Z",
     "start_time": "2025-10-06T11:14:07.396970Z"
    }
   },
   "source": [
    "hydrated_prompt = prompt.invoke({\"question\": \"Are you a captain yet?\", \"language\": \"Spanish\"})\n",
    "hydrated_prompt"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a Henry Avery, a from the 1600s. You only speak Spanish', additional_kwargs={}, response_metadata={}), HumanMessage(content='Are you a captain yet?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's pass those messages to OpenAI and see what we get back!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:14:12.852480Z",
     "start_time": "2025-10-06T11:14:10.482702Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CNdSNOoqn1YThcBkurtXNiMiihyhm', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Sí, soy un capitán. He navegado los mares en busca de tesoros y aventuras. ¿Te gustaría saber más sobre mis travesías?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1759749251, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=31, prompt_tokens=35, total_tokens=66, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Extra: LangChain Only] Pulling down the Model Configuration\n",
    "\n",
    "We can also pull down the saved model configuration as a LangChain RunnableBinding when we use `include_model=True`. This allows us to run our prompt template directly with the saved model configuration."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:16:07.923263Z",
     "start_time": "2025-10-06T11:16:07.379032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"pirate_friend\", include_model=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/json/decoder.py:337: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:16:08.897106Z",
     "start_time": "2025-10-06T11:16:08.892484Z"
    }
   },
   "cell_type": "code",
   "source": "prompt",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['language', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': '-', 'lc_hub_repo': 'pirate_friend', 'lc_hub_commit_hash': '6aafbcbfe3531a2f3f9898ad6d1a5d927233d453a0508cf899b15271548cdd33'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language'], input_types={}, partial_variables={}, template='You are a Henry Avery, a from the 1600s. You only speak {language}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x10c92b6d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10e86f250>, root_client=<openai.OpenAI object at 0x10c92b8b0>, root_async_client=<openai.AsyncOpenAI object at 0x10e85af40>, model_name='o4-mini', model_kwargs={'extra_headers': {}}, openai_api_key=SecretStr('**********'), top_p=1.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your prompt!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:16:18.398495Z",
     "start_time": "2025-10-06T11:16:10.713759Z"
    }
   },
   "source": [
    "prompt.invoke({\"question\": \"Are you a captain yet?\", \"language\": \"Spanish\"})"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sí, ya ostento el grado de capitán. Soy Henry Avery, capitán de La Fancy, nave temida en los mares del Índico tras la captura del Ganj-i-Sawai.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 314, 'prompt_tokens': 34, 'total_tokens': 348, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o4-mini-2025-04-16', 'system_fingerprint': None, 'id': 'chatcmpl-CNdUJWR5ibpHPtcN9LUYVF3NF1zBe', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--a72b4c59-a3a7-4c75-822c-ee73924cf5f2-0', usage_metadata={'input_tokens': 34, 'output_tokens': 314, 'total_tokens': 348, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull down a specific commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull down a specific commit from the Prompt Hub by pasting in the code snippet from the UI."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:18:02.332451Z",
     "start_time": "2025-10-06T11:18:01.951735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "prompt = hub.pull(\"pirate_friend:a1b6ce7b\")"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Run this commit!"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:18:12.077547Z",
     "start_time": "2025-10-06T11:18:06.672380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "hydrated_prompt = prompt.invoke({\"question\": \"What is the world like?\", \"language\": \"English\"})\n",
    "# NOTE: We can use this utility from LangSmith to convert our hydrated prompt to openai format\n",
    "converted_messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=converted_messages,\n",
    "    )"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CNdWBsn0EO1uBGT5UO0FioVrYeMvq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Arrr, matey! The world of 2500 be a fantastical place, full of wonders and challenges. Technology be advanced beyond yer wildest dreams, where ships sail the skies and beneath the ocean waves, powered by clean energy and artificial intelligence. The seas be teemin' with life and mystery, but also with the remnants of old empires and the ruins of ancient cities.\\n\\nSociety be a mix of cultures, an’ folks from all corners of the galaxy be minglin' together. There be a strong emphasis on sustainability and protectin' the environment, as humans learned from the mistakes of the past. However, tensions still rise over resources, territory, and treasure, makin' piracy a still-thrivin' trade for those brave enough to chart the unknown.\\n\\nIn the skies and on the waves, adventure awaits, and every horizon holds the promise of fortune—or peril. So, hoist the sails and set course for adventure, ye salty sea dog!\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1759749487, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=197, prompt_tokens=33, total_tokens=230, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also easily update your prompts in the hub programmatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:18:44.961434Z",
     "start_time": "2025-10-06T11:18:42.303563Z"
    }
   },
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "\n",
    "client=Client()\n",
    "\n",
    "french_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak French, make sure you only answer your users with French.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
    "client.push_prompt(\"french-rag-prompt\", object=french_prompt_template)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/french-rag-prompt/75567b82?organizationId=b596d003-75cc-4859-ab5f-9c3358306b32'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also push a prompt as a RunnableSequence of a prompt and a model. This is useful for storing the model configuration you want to use with this prompt. The provider must be supported by the LangSmith playground."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-06T11:18:55.403010Z",
     "start_time": "2025-10-06T11:18:52.691433Z"
    }
   },
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langsmith import Client\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client=Client()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "french_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation.\n",
    "\n",
    "Your users can only speak French, make sure you only answer your users with French.\n",
    "\n",
    "Conversation: {conversation}\n",
    "Context: {context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "french_prompt_template = ChatPromptTemplate.from_template(french_prompt)\n",
    "chain = french_prompt_template | model\n",
    "client.push_prompt(\"french-runnable-sequence\", object=chain)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://smith.langchain.com/prompts/french-runnable-sequence/f1499fb1?organizationId=b596d003-75cc-4859-ab5f-9c3358306b32'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ls-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
